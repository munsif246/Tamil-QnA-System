{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T08:35:14.926840Z",
     "iopub.status.busy": "2023-01-07T08:35:14.926165Z",
     "iopub.status.idle": "2023-01-07T08:35:22.220704Z",
     "shell.execute_reply": "2023-01-07T08:35:22.219772Z",
     "shell.execute_reply.started": "2023-01-07T08:35:14.926735Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import AutoTokenizer,AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from transformers import default_data_collator\n",
    "import collections\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T08:35:22.223940Z",
     "iopub.status.busy": "2023-01-07T08:35:22.223314Z",
     "iopub.status.idle": "2023-01-07T08:35:32.261907Z",
     "shell.execute_reply": "2023-01-07T08:35:32.260906Z",
     "shell.execute_reply.started": "2023-01-07T08:35:22.223899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-cloud 0.1.13 requires tensorflow<3.0,>=1.15.0, which is not installed.\n",
      "dask-cudf 21.6.1+2.g101fc0fda4 requires cupy-cuda112, which is not installed.\n",
      "cudf 21.6.1+2.g101fc0fda4 requires cupy-cuda110, which is not installed.\n",
      "s3fs 2021.6.1 requires fsspec==2021.06.1, but you have fsspec 2021.6.0 which is incompatible.\n",
      "pytorch-lightning 1.3.8 requires fsspec[http]!=2021.06.0,>=2021.05.0, but you have fsspec 2021.6.0 which is incompatible.\n",
      "dask-cudf 21.6.1+2.g101fc0fda4 requires dask<=2021.5.1,>=2021.4.0, but you have dask 2021.6.2 which is incompatible.\n",
      "dask-cudf 21.6.1+2.g101fc0fda4 requires distributed<=2021.5.1,>=2.22.0, but you have distributed 2021.6.2 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall fsspec -qq -y\n",
    "!pip install --no-index --find-links ../hf-datasets/wheels datasets -qq\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T08:35:32.269468Z",
     "iopub.status.busy": "2023-01-07T08:35:32.267179Z",
     "iopub.status.idle": "2023-01-07T08:35:32.280650Z",
     "shell.execute_reply": "2023-01-07T08:35:32.279531Z",
     "shell.execute_reply.started": "2023-01-07T08:35:32.269418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_DISABLED=True\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_DISABLED=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T09:52:04.366875Z",
     "iopub.status.busy": "2023-01-07T09:52:04.366518Z",
     "iopub.status.idle": "2023-01-07T09:52:04.384904Z",
     "shell.execute_reply": "2023-01-07T09:52:04.383928Z",
     "shell.execute_reply.started": "2023-01-07T09:52:04.366820Z"
    }
   },
   "outputs": [],
   "source": [
    "class preprocess:\n",
    "    def __init__(self,path):\n",
    "        self.path = path\n",
    "    \n",
    "    def import_dataset(self):\n",
    "        df = pd.read_csv(self.path)\n",
    "        return df\n",
    "    \n",
    "    def clean_df(self):\n",
    "        df = self.import_dataset()\n",
    "        df['isnull'] = df.context.isnull()\n",
    "        df = df[df['isnull'] !=True]\n",
    "        df = df.reset_index(drop=True)\n",
    "        self.df = df\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def no_of_uniq_ids(self):\n",
    "        df = self.clean_df()\n",
    "        return df.id.nunique()\n",
    "    \n",
    "    def import_tokenizer(self, checkpoint):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        self.tokenizer = tokenizer\n",
    "        return tokenizer\n",
    "    \n",
    "    def token_count_distribution(self, tokenizer):\n",
    "        df = self.clean_df()\n",
    "        df['num_tokens_context'] = df['context'].apply(lambda t: len(tokenizer(t)['input_ids']))\n",
    "        return df['num_tokens_context'].hist()\n",
    "    \n",
    "    def convert_answers(self,r):\n",
    "        start = r[0]\n",
    "        text = r[1]\n",
    "        return {\n",
    "            'answer_start': [start],\n",
    "            'text': [text]\n",
    "        }\n",
    "    \n",
    "    def shuffle(self):\n",
    "        df = self.clean_df()\n",
    "        df = df.sample(frac=1, random_state=42)\n",
    "        df['answers'] = df[['answer_start', 'answer_text']].apply(self.convert_answers, axis=1)\n",
    "        return df\n",
    "        \n",
    "    def split_data(self):\n",
    "        df = self.shuffle()\n",
    "        df_train = df[:-64].reset_index(drop=True)\n",
    "        df_valid = df[-64:].reset_index(drop=True)\n",
    "        train_dataset = Dataset.from_pandas(df_train)\n",
    "        valid_dataset = Dataset.from_pandas(df_valid)\n",
    "        \n",
    "        return train_dataset,valid_dataset\n",
    "    \n",
    "    \n",
    "    def prepare_train_features(self,examples):\n",
    "        max_length = 384 \n",
    "        doc_stride = 128\n",
    "        pad_on_right = tokenizer.padding_side == \"right\"\n",
    "    \n",
    "        examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "        tokenized_examples = tokenizer(\n",
    "            examples[\"question\" if pad_on_right else \"context\"],\n",
    "            examples[\"context\" if pad_on_right else \"question\"],\n",
    "            truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "            max_length=max_length,\n",
    "            stride=doc_stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    \n",
    "        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "        tokenized_examples[\"start_positions\"] = []\n",
    "        tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "        for i, offsets in enumerate(offset_mapping):\n",
    "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "            cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "            sample_index = sample_mapping[i]\n",
    "            answers = examples[\"answers\"][sample_index]\n",
    "            if len(answers[\"answer_start\"]) == 0:\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                start_char = answers[\"answer_start\"][0]\n",
    "                end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "                token_start_index = 0\n",
    "                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                    token_start_index += 1\n",
    "\n",
    "                token_end_index = len(input_ids) - 1\n",
    "                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                    token_end_index -= 1\n",
    "\n",
    "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                    tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                    tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "                else:\n",
    "                    \n",
    "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                        token_start_index += 1\n",
    "                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                    while offsets[token_end_index][1] >= end_char:\n",
    "                        token_end_index -= 1\n",
    "                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "        return tokenized_examples\n",
    "        \n",
    "    def tokenized_df(self):\n",
    "        train_dataset,valid_dataset = self.split_data()\n",
    "        tokenized_train_ds = train_dataset.map(self.prepare_train_features, batched=True, remove_columns=train_dataset.column_names)\n",
    "        tokenized_valid_ds = valid_dataset.map(self.prepare_train_features, batched=True, remove_columns=train_dataset.column_names)\n",
    "        return tokenized_train_ds,tokenized_valid_ds\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T09:52:06.985285Z",
     "iopub.status.busy": "2023-01-07T09:52:06.984952Z",
     "iopub.status.idle": "2023-01-07T09:52:24.934638Z",
     "shell.execute_reply": "2023-01-07T09:52:24.933633Z",
     "shell.execute_reply.started": "2023-01-07T09:52:06.985255Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f99f4728744b4aaee54278cfc29f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a0e339ae454f5ea9b885d0328abc59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocess2 = preprocess('df_tam3.csv')\n",
    "tokenizer = preprocess2.import_tokenizer('xlm-roberta-base-squad2')\n",
    "train_dataset,valid_dataset = preprocess2.split_data()\n",
    "tokenized_train_ds,tokenized_valid_ds = preprocess2.tokenized_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T09:52:24.937109Z",
     "iopub.status.busy": "2023-01-07T09:52:24.936472Z",
     "iopub.status.idle": "2023-01-07T09:52:41.850592Z",
     "shell.execute_reply": "2023-01-07T09:52:41.849698Z",
     "shell.execute_reply.started": "2023-01-07T09:52:24.937061Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1492 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocess2.token_count_distribution(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T08:35:51.056600Z",
     "iopub.status.busy": "2023-01-07T08:35:51.056054Z",
     "iopub.status.idle": "2023-01-07T08:35:51.063683Z",
     "shell.execute_reply": "2023-01-07T08:35:51.062997Z",
     "shell.execute_reply.started": "2023-01-07T08:35:51.056560Z"
    }
   },
   "outputs": [],
   "source": [
    "class model_building:\n",
    "    \n",
    "    def __init__(self,model_checkpoint):\n",
    "        self.model_checkpoint = model_checkpoint\n",
    "    \n",
    "    def import_model(self):\n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(self.model_checkpoint)\n",
    "        return model\n",
    "    \n",
    "    def training_parameters(self,learning_rate=2e-5,batch_size = 4,weight_decay=0.01,num_train_epochs=1):\n",
    "        args = TrainingArguments(\n",
    "            f\"tamil-qa\",\n",
    "            evaluation_strategy = \"epoch\",\n",
    "            save_strategy = \"epoch\",\n",
    "            learning_rate=learning_rate,\n",
    "            warmup_ratio=0.1,\n",
    "            gradient_accumulation_steps=8,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "        return args\n",
    "    \n",
    "    def train_model(self, model, args, tokenized_train_ds,tokenized_valid_ds):\n",
    "        data_collator = default_data_collator\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            args,\n",
    "            train_dataset=tokenized_train_ds,\n",
    "            eval_dataset=tokenized_valid_ds,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "        trainer.train()\n",
    "        \n",
    "        return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T08:35:51.065348Z",
     "iopub.status.busy": "2023-01-07T08:35:51.064802Z",
     "iopub.status.idle": "2023-01-07T09:26:03.525614Z",
     "shell.execute_reply": "2023-01-07T09:26:03.524764Z",
     "shell.execute_reply.started": "2023-01-07T08:35:51.065313Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1910' max='1910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1910/1910 49:47, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.188149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.970500</td>\n",
       "      <td>1.145816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.637100</td>\n",
       "      <td>1.294962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.426600</td>\n",
       "      <td>1.305490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.426600</td>\n",
       "      <td>1.434978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelbuild = model_building('xlm-roberta-squad2/deepset/xlm-roberta-base-squad2')\n",
    "model = modelbuild.import_model()\n",
    "args = modelbuild.training_parameters(num_train_epochs=5)\n",
    "trainer = modelbuild.train_model(model, args, tokenized_train_ds,tokenized_valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T09:26:03.528502Z",
     "iopub.status.busy": "2023-01-07T09:26:03.528181Z",
     "iopub.status.idle": "2023-01-07T09:26:03.546540Z",
     "shell.execute_reply": "2023-01-07T09:26:03.545530Z",
     "shell.execute_reply.started": "2023-01-07T09:26:03.528468Z"
    }
   },
   "outputs": [],
   "source": [
    "class answer_extraction:\n",
    "    \n",
    "    def __init__(self, valid_dataset):\n",
    "        self.valid_dataset = valid_dataset\n",
    "    \n",
    "    def prepare_validation_features(self,examples):\n",
    "        max_length = 384 \n",
    "        doc_stride = 128\n",
    "        pad_on_right = tokenizer.padding_side == \"right\"\n",
    "        \n",
    "        examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "        tokenized_examples = tokenizer(\n",
    "            examples[\"question\" if pad_on_right else \"context\"],\n",
    "            examples[\"context\" if pad_on_right else \"question\"],\n",
    "            truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "            max_length=max_length,\n",
    "            stride=doc_stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "        tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "        for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "            context_index = 1 if pad_on_right else 0\n",
    "\n",
    "            sample_index = sample_mapping[i]\n",
    "            tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "            tokenized_examples[\"offset_mapping\"][i] = [\n",
    "                (o if sequence_ids[k] == context_index else None)\n",
    "                for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "            ]\n",
    "\n",
    "        return tokenized_examples\n",
    "    \n",
    "    def apply_validation_feature(self):\n",
    "        valid_dataset = self.valid_dataset\n",
    "        validation_features = valid_dataset.map(\n",
    "            self.prepare_validation_features,\n",
    "            batched=True,\n",
    "            remove_columns=valid_dataset.column_names\n",
    "        )\n",
    "        return validation_features\n",
    "        \n",
    "    def valid(self):\n",
    "        validation_features = self.apply_validation_feature()\n",
    "        valid_feats_small = validation_features.map(lambda example: example, remove_columns=['example_id', 'offset_mapping'])\n",
    "        return valid_feats_small\n",
    "    \n",
    "    def raw_predict(self,trainer):\n",
    "        valid_feats_small = self.valid()\n",
    "        raw_predictions = trainer.predict(valid_feats_small)\n",
    "        return raw_predictions\n",
    "    \n",
    "    def postprocess_qa_predictions(self, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "        examples = self.valid_dataset\n",
    "        features = self.apply_validation_feature()\n",
    "        \n",
    "        all_start_logits, all_end_logits = raw_predictions\n",
    "        example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "        features_per_example = collections.defaultdict(list)\n",
    "        \n",
    "        for i, feature in enumerate(features):\n",
    "            features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "        predictions = collections.OrderedDict()\n",
    "\n",
    "        print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
    "\n",
    "        for example_index, example in enumerate(tqdm(examples)):\n",
    "            feature_indices = features_per_example[example_index]\n",
    "\n",
    "            min_null_score = None \n",
    "            valid_answers = []\n",
    "\n",
    "            context = example[\"context\"]\n",
    "            for feature_index in feature_indices: \n",
    "                start_logits = all_start_logits[feature_index]\n",
    "                end_logits = all_end_logits[feature_index]\n",
    "              \n",
    "                offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "                cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "                feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "                if min_null_score is None or min_null_score < feature_null_score:\n",
    "                    min_null_score = feature_null_score\n",
    "\n",
    "                start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "                end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "                for start_index in start_indexes:\n",
    "                    for end_index in end_indexes:\n",
    "                        if (\n",
    "                            start_index >= len(offset_mapping)\n",
    "                            or end_index >= len(offset_mapping)\n",
    "                            or offset_mapping[start_index] is None\n",
    "                            or offset_mapping[end_index] is None\n",
    "                        ):\n",
    "                            continue\n",
    "                        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                            continue\n",
    "\n",
    "                        start_char = offset_mapping[start_index][0]\n",
    "                        end_char = offset_mapping[end_index][1]\n",
    "                        valid_answers.append(\n",
    "                            {\n",
    "                                \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                                \"text\": context[start_char: end_char]\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "            if len(valid_answers) > 0:\n",
    "                best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "            else:\n",
    "               \n",
    "                best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T09:26:03.548414Z",
     "iopub.status.busy": "2023-01-07T09:26:03.547930Z",
     "iopub.status.idle": "2023-01-07T09:26:05.228568Z",
     "shell.execute_reply": "2023-01-07T09:26:05.227361Z",
     "shell.execute_reply.started": "2023-01-07T09:26:03.548377Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_exact_match(row):\n",
    "    return int(row[0] == row[1])\n",
    "\n",
    "def jaccard(row): \n",
    "    str1 = row[0]\n",
    "    str2 = row[1]\n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def compute_f1(row):\n",
    "    truth = row[0]\n",
    "    prediction = row[1]\n",
    "    pred_tokens = prediction.split()\n",
    "    truth_tokens = truth.split()\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    \n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "    \n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "    \n",
    "    return 2 * (prec * rec) / (prec + rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T09:26:05.273301Z",
     "iopub.status.busy": "2023-01-07T09:26:05.263225Z",
     "iopub.status.idle": "2023-01-07T09:26:11.059878Z",
     "shell.execute_reply": "2023-01-07T09:26:11.058680Z",
     "shell.execute_reply.started": "2023-01-07T09:26:05.273255Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47d7539ee284137ac8d5e2186dc7e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112d5b26bebe42c49ec817b4bc2b355c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46ae42664b147f583d574a9bf9388e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/169 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872d03d54ab34524a4beb62cee69643c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ccbbdcc35c4423b49d5681668fbb18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/169 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='43' max='43' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [43/43 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testing2 = answer_extraction(valid_dataset)\n",
    "validation_features = testing2.apply_validation_feature()\n",
    "valid_feats_small = testing2.valid()\n",
    "raw_predictions = testing2.raw_predict(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T09:26:11.068229Z",
     "iopub.status.busy": "2023-01-07T09:26:11.065545Z",
     "iopub.status.idle": "2023-01-07T09:26:12.584308Z",
     "shell.execute_reply": "2023-01-07T09:26:12.583552Z",
     "shell.execute_reply.started": "2023-01-07T09:26:11.068175Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39c3f84a59144b28a3a7d13f37d4a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 64 example predictions split into 169 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64e3205c67149c8bfda3e7f426b1733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = testing2.postprocess_qa_predictions( \n",
    "                                    raw_predictions.predictions, n_best_size = 20, \n",
    "                                    max_answer_length = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T09:26:12.589760Z",
     "iopub.status.busy": "2023-01-07T09:26:12.587849Z",
     "iopub.status.idle": "2023-01-07T09:26:12.911083Z",
     "shell.execute_reply": "2023-01-07T09:26:12.906659Z",
     "shell.execute_reply.started": "2023-01-07T09:26:12.589719Z"
    }
   },
   "outputs": [],
   "source": [
    "references = [{\"id\": ex[\"id\"], \"answer_text\": ex[\"answers\"]['text'][0]} for ex in valid_dataset]\n",
    "res = pd.DataFrame(references)\n",
    "res['prediction'] = res['id'].apply(lambda r: predictions[r])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T09:31:20.866629Z",
     "iopub.status.busy": "2023-01-07T09:31:20.866307Z",
     "iopub.status.idle": "2023-01-07T09:31:20.889033Z",
     "shell.execute_reply": "2023-01-07T09:31:20.888092Z",
     "shell.execute_reply.started": "2023-01-07T09:31:20.866596Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>prediction</th>\n",
       "      <th>jaccard</th>\n",
       "      <th>EM</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>e693ffa1</td>\n",
       "      <td>720 களுக்குப் பிறகு</td>\n",
       "      <td>கிமு 720</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>83f94fe8</td>\n",
       "      <td>உயிரியல்</td>\n",
       "      <td>தாமிரம் உயிரியல்</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>08fc5f48</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>28f9e6ea</td>\n",
       "      <td>கோல்கொண்டா</td>\n",
       "      <td>கோல்கொண்டா</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1fc78dff</td>\n",
       "      <td>ஹிரூ</td>\n",
       "      <td>ஹிரூ</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id          answer_text        prediction  jaccard  EM        F1\n",
       "59  e693ffa1  720 களுக்குப் பிறகு          கிமு 720     0.25   0  0.400000\n",
       "60  83f94fe8             உயிரியல்  தாமிரம் உயிரியல்     0.50   0  0.666667\n",
       "61  08fc5f48                   11                11     1.00   0  1.000000\n",
       "62  28f9e6ea           கோல்கொண்டா        கோல்கொண்டா     1.00   0  1.000000\n",
       "63  1fc78dff                 ஹிரூ              ஹிரூ     1.00   0  1.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['jaccard'] = res[['answer_text', 'prediction']].apply(jaccard, axis=1)\n",
    "res ['EM'] = res[['answer_text', 'prediction']].apply(compute_exact_match, axis=1)\n",
    "res ['F1'] = res[['answer_text', 'prediction']].apply(compute_f1, axis=1)\n",
    "res.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-07T09:31:21.657643Z",
     "iopub.status.busy": "2023-01-07T09:31:21.657319Z",
     "iopub.status.idle": "2023-01-07T09:31:21.665556Z",
     "shell.execute_reply": "2023-01-07T09:31:21.664549Z",
     "shell.execute_reply.started": "2023-01-07T09:31:21.657610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.5772845643939393\n",
      "Jaccard score:  0.5378348214285714\n",
      "Exact match score:  0.125\n"
     ]
    }
   ],
   "source": [
    "print('F1 Score: ',res.F1.mean())\n",
    "print('Jaccard score: ',res.jaccard.mean())\n",
    "print('Exact match score: ', res.EM.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
